{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMPZKNsqE5BGXCh8mzPYDpr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"id":"724mEXt-G-N3","executionInfo":{"status":"ok","timestamp":1663092736430,"user_tz":-120,"elapsed":5897,"user":{"displayName":"Tim Schäfer","userId":"03475117311436082468"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a8646a4-7a0b-4b5b-961e-17f48052bb2f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"," abide_freesurfer_lgi_2persite\n"," abide_freesurfer_lgi_predict\n","'Colab Notebooks'\n","'Copy of Teampaper_BrainMap_for_export.gdoc'\n"," Export_Copy_of_Teampaper_BrainMap_do_not_edit_online.gdoc\n"," Export_Teampaper_BrainMap_Export_noen_nocomments.gdoc\n","'Power in sMRI.gslides'\n"," Teampaper_BrainMap_Export_noen_nocomments.gdoc\n"]}],"source":["# Run a Scikit-Learn Random Forest Regressor on the ABIDE FreeSurfer meshes,\n","# to predict the local gyrification index at each vertex.\n","\n","import numpy as np\n","import pandas as pd\n","import psutil\n","import sys\n","import os\n","import time\n","from datetime import timedelta\n","import psutil\n","from sys import getsizeof\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn import metrics\n","\n","from meshlearn.training_data import TrainingData, get_valid_mesh_desc_file_pairs_reconall\n","\n","\n","# Prepare to access the data, stored in Google Drive of the \n","# Colab user (same Google Account).\n","from google.colab import drive\n","drive.mount('/content/drive') # This will prompt for authorization.\n","\n","#!ls \"/content/drive/My Drive\" # Show data in Google Drive, to be sure it worked.\n","\n","\n"]},{"cell_type":"code","source":["\n","# Install meshlearn. The '-q' is for quiet, to prevent long output in notebook.\n","!pip install -q git+https://github.com/dfsp-spirit/meshlearn.git\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"onHzxuXvrNIG","executionInfo":{"status":"ok","timestamp":1663092748821,"user_tz":-120,"elapsed":6772,"user":{"displayName":"Tim Schäfer","userId":"03475117311436082468"}},"outputId":"fc103534-0b0d-4431-8bcf-c7081bd39990"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/dfsp-spirit/meshlearn.git\n","  Cloning https://github.com/dfsp-spirit/meshlearn.git to /tmp/pip-req-build-y6vnu5t6\n","  Running command git clone -q https://github.com/dfsp-spirit/meshlearn.git /tmp/pip-req-build-y6vnu5t6\n","Requirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.7/dist-packages (from meshlearn==0.0.1) (2.8.2+zzzcolab20220719082949)\n","Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (from meshlearn==0.0.1) (4.6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from meshlearn==0.0.1) (1.21.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from meshlearn==0.0.1) (3.2.2)\n","Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (from meshlearn==0.0.1) (3.0.2)\n","Requirement already satisfied: trimesh in /usr/local/lib/python3.7/dist-packages (from meshlearn==0.0.1) (3.14.1)\n","Requirement already satisfied: brainload in /usr/local/lib/python3.7/dist-packages (from meshlearn==0.0.1) (0.3.5)\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from meshlearn==0.0.1) (0.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (1.1.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (1.15.0)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (2.8.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (3.3.0)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (2.0.7)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (1.48.1)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (0.5.3)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (0.26.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (1.1.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (57.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (0.2.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (14.0.6)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (3.1.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (1.6.3)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (2.8.0)\n","Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (2.8.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (3.17.3)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (1.2.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (4.1.1)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->meshlearn==0.0.1) (1.14.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0->meshlearn==0.0.1) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.0->meshlearn==0.0.1) (1.5.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (3.4.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (1.0.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (1.35.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (0.6.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (4.9)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (3.8.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.0->meshlearn==0.0.1) (3.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->meshlearn==0.0.1) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->meshlearn==0.0.1) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->meshlearn==0.0.1) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->meshlearn==0.0.1) (3.0.9)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->meshlearn==0.0.1) (1.0.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->meshlearn==0.0.1) (1.7.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->meshlearn==0.0.1) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->meshlearn==0.0.1) (3.1.0)\n","Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->meshlearn==0.0.1) (0.7.1)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->meshlearn==0.0.1) (1.10.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->meshlearn==0.0.1) (5.9.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->meshlearn==0.0.1) (0.3.5.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->meshlearn==0.0.1) (4.64.1)\n","Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->meshlearn==0.0.1) (2.3)\n","Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->meshlearn==0.0.1) (0.10.2)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets->meshlearn==0.0.1) (1.56.4)\n"]}]},{"cell_type":"code","source":["\n","\"\"\"\n","Train and evaluate an lGI prediction model.\n","\"\"\"\n","\n","### Settings\n","\n","data_dir=\"/content/drive/My Drive/abide_freesurfer_lgi_2persite\"\n","neigh_count = 300  # Number of vertices to consider at max in the edge neighborhoods for Euclidean dist\n","neigh_radius = 10  #  Radius for sphere for Euclidean dist, in spatial units of mesh (e.g., mm).\n","load_max = 40_000  # Number of samples to load. Set to 0 for all data in the files discovered in the data_dir.\n","verbose = True\n","\n","### End of Settings\n","\n","mesh_neighborhood_count = neigh_count # How many vertices in the edge neighborhood do we consider (the 'local' neighbors from which we learn).\n","mesh_neighborhood_radius = neigh_radius\n","\n","num_neighborhoods_to_load = None if int(load_max) == 0 else int(load_max)\n","\n","print(\"---Train and evaluate an lGI prediction model---\")\n","if verbose:\n","    print(\"Verbosity turned on.\")\n","\n","print(f\"Using data directory '{data_dir}', observations to load limit is set to: {num_neighborhoods_to_load}.\")\n","print(f\"Using neighborhood radius {mesh_neighborhood_radius} and keeping {mesh_neighborhood_count} vertices per neighborhood.\")\n","\n","if num_neighborhoods_to_load is not None:\n","    # Estimate total dataset size in RAM early to prevent crashing later, if possible.\n","    ds_estimated_num_values_per_neighborhood = 6 * mesh_neighborhood_count + 1\n","    ds_estimated_num_neighborhoods = num_neighborhoods_to_load\n","    # try to allocate, will err if too little RAM.\n","    print(f\"RAM available is about {int(psutil.virtual_memory().available / 1024. / 1024.)} MB\")\n","    ds_dummy = np.empty((ds_estimated_num_neighborhoods, ds_estimated_num_values_per_neighborhood))\n","    ds_estimated_full_data_size_bytes = getsizeof(ds_dummy)\n","    ds_dummy = None\n","    ds_estimated_full_data_size_MB = ds_estimated_full_data_size_bytes / 1024. / 1024.\n","    print(f\"Estimated dataset size in RAM will be {int(ds_estimated_full_data_size_MB)} MB.\")\n","\n","\n","\n","discover_start = time.time()\n","mesh_files, desc_files, cortex_files, val_subjects = get_valid_mesh_desc_file_pairs_reconall(data_dir)\n","discover_end = time.time()\n","discover_execution_time = discover_end - discover_start\n","print(f\"=== Discovering data files done, it took: {timedelta(seconds=discover_execution_time)} ===\")\n","\n","### Decide which files are used as training, validation and test data. ###\n","input_file_dict = dict(zip(mesh_files, desc_files))\n","\n","if verbose:\n","    print(f\"Discovered {len(input_file_dict)} valid pairs of input mesh and descriptor files.\")\n","\n","if num_neighborhoods_to_load is None:\n","    print(f\"Will load all data from the {len(input_file_dict)} files.\")\n","else:\n","    print(f\"Will load {num_neighborhoods_to_load} samples in total from the {len(input_file_dict)} files.\")\n","\n","load_start = time.time()\n","tdl = TrainingData(neighborhood_radius=mesh_neighborhood_radius, num_neighbors=mesh_neighborhood_count)\n","dataset, col_names = tdl.load_raw_data(input_file_dict, num_samples_to_load=num_neighborhoods_to_load)\n","load_end = time.time()\n","load_execution_time = load_end - load_start\n","print(f\"=== Loading data files done, it took: {timedelta(seconds=load_execution_time)} ===\")\n","\n","assert isinstance(dataset, pd.DataFrame)\n","\n","print(\"Separating observations and labels...\")\n","\n","nc = len(dataset.columns)\n","X = dataset.iloc[:, 0:(nc-1)].values\n","y = dataset.iloc[:, (nc-1)].values\n","\n","print(\"Splitting data into train and test sets...\")\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","\n","print(f\"Created training data set with shape {X_train.shape} and testing data set with shape {X_test.shape}.\")\n","print(f\"The label arrays have shape {y_train.shape} for the training data and  {y_test.shape} for the testing data.\")\n","\n","\n","print(\"Scaling...\")\n","\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)\n","\n","\n","\n","n_estimators = 100\n","\n","\n","fit_start = time.time()\n","print(f\"Fitting with RandomForestRegressor with {n_estimators} estimators. (Started at {fit_start}.)\")\n","\n","regressor = RandomForestRegressor(n_estimators=n_estimators, random_state=0, n_jobs=-1)\n","regressor.fit(X_train, y_train)\n","\n","fit_end = time.time()\n","fit_execution_time = fit_end - fit_start\n","\n","print(f\"===Fitting done, it took: {timedelta(seconds=fit_execution_time)} ===\")\n","print(f\"Using trained model to predict for test data set with shape {X_test.shape}.\")\n","\n","y_pred = regressor.predict(X_test)\n","\n","\n","print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n","print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n","print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n","\n","# Evaluate feature importance\n","importances = regressor.feature_importances_\n","\n","feature_names = col_names[:-1]\n","\n","print(f\"=== Evaluating Feature importance ===\")\n","print(f\"Feature names: {feature_names}\")\n","print(f\"Feature impor: {importances}\")\n","\n","do_plot = False\n","if do_plot:\n","    std = np.std([tree.feature_importances_ for tree in regressor.estimators_], axis=0)\n","    forest_importances = pd.Series(importances, index=feature_names)\n","    import matplotlib.pyplot as plt\n","    fig, ax = plt.subplots()\n","    forest_importances.plot.bar(yerr=std, ax=ax)\n","    ax.set_title(\"Feature importances using MDI\")\n","    ax.set_ylabel(\"Mean decrease in impurity\")\n","    fig.tight_layout()\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HpinLK4Dt_E0","outputId":"6a6098ee-3809-4838-94cd-2a6efdfa41b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---Train and evaluate an lGI prediction model---\n","Verbosity turned on.\n","Using data directory '/content/drive/My Drive/abide_freesurfer_lgi_2persite', observations to load limit is set to: 40000.\n","Using neighborhood radius 10 and keeping 300 vertices per neighborhood.\n","RAM available is about 11698 MB\n","Estimated dataset size in RAM will be 549 MB.\n","Using subjects list containing 32 subjects. Loading them from recon-all output dir '/content/drive/My Drive/abide_freesurfer_lgi_2persite'.\n","Discovering surface 'pial', descriptor 'pial_lgi' for 2 hemis: ['lh', 'rh'].\n","Not discovering cortex labels.\n","Out of 64 subject hemispheres (32 subjects), 63 had the requested surface and descriptor files.\n","=== Discovering data files done, it took: 0:00:15.269974 ===\n","Discovered 63 valid pairs of input mesh and descriptor files.\n","Will load 40000 samples in total from the 63 files.\n","[load] Loading data.\n","[load] * Loading mesh file '/content/drive/My Drive/abide_freesurfer_lgi_2persite/Caltech_0051456/surf/lh.pial' and descriptor file '/content/drive/My Drive/abide_freesurfer_lgi_2persite/Caltech_0051456/surf/lh.pial_lgi'.\n","[load]  - Computing neighborhoods based on radius 10 for 152304 vertices in mesh file '/content/drive/My Drive/abide_freesurfer_lgi_2persite/Caltech_0051456/surf/lh.pial'.\n","Min neigh size across 152304 neighborhoods is 188, max is 1897, mean is 911.7895918688938, median is 905.0\n","Filtered neighborhoods, 151891 of 152304 left after removing all smaller than 300 verts\n","[load]  - Current neighborhood #1 size in RAM is about 2194396152 bytes, or 2092.7392501831055 MB.\n","[load] Done loading the requested 40000 samples, ignoring the rest.\n","[load] Returning 152304 instead of 40000 samples, file contained more and 'force_no_more_than_num_samples_to_load' is false.\n","[load] Total dataset size in RAM is about 2194396160 bytes, or 2092 MB.\n","[load] RAM available is about 9310 MB\n","=== Loading data files done, it took: 0:19:13.490753 ===\n","Separating observations and labels...\n","Splitting data into train and test sets...\n","Created training data set with shape (121843, 1800) and testing data set with shape (30461, 1800).\n","The label arrays have shape (121843,) for the training data and  (30461,) for the testing data.\n","Scaling...\n","Fitting with RandomForestRegressor with 100 estimators. (Started at 1663096925.4943128.)\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"cnvvSW-gtWgY"}}]}